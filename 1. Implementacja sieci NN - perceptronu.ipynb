{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testowe dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE0TsuRnjnna"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array([[10,11], [20,30], [5,4], [8,20], [22, 10], [9, 10], [5, 17], [5, 50], [50,5], [3,4], [6,7], [-10,10], [-10,1]])\n",
    "y_train=np.array([0,1, 0, 1, 0, 0, 1,1,0,0,0, 1,1])\n",
    "assert len(X_train)==len(y_train)\n",
    "\n",
    "\n",
    "X_test=np.array([[0,10], [20,30], [-20, 1], [19,20], [10,5], [2,1]])\n",
    "y_test=np.array([1,1,1,0,0,0])\n",
    "assert len(X_test)==len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementacja perceptronu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, num_features, num_epoch = 1000, learning_rate = 0.01):\n",
    "        self.num_features = num_features\n",
    "        self.num_epoch = num_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(0, 1, self.num_features + 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def __forward(self, x):\n",
    "        y_prob = self.sigmoid(np.dot(x, self.weights[1:]) + self.weights[0])\n",
    "        return y_prob\n",
    "    \n",
    "    def train(self, X, Y, X_test, Y_test):\n",
    "        \n",
    "        for epoch in range(self.num_epoch):\n",
    "            loss = 0\n",
    "            y_true = 0\n",
    "            \n",
    "            for x, y in zip(X, Y):\n",
    "                y_prob = self.__forward(x)\n",
    "                y_pred = 1 if y_prob > 0.5 else 0\n",
    "                if y_pred == y:\n",
    "                    y_true += 1\n",
    "                loss += 0.5 * (y - y_prob) ** 2\n",
    "                self.weights[1:] += self.learning_rate * (y - y_prob) * y_prob * (1 - y_prob) * x\n",
    "                self.weights[0] += self.learning_rate * (y - y_prob) * y_prob * (1 - y_prob)\n",
    "                \n",
    "            pred = self.predict(X_test)\n",
    "            acc = sum([ y == y_ for y, y_ in zip(pred, Y_test)]) / Y_test.shape[0]\n",
    "            \n",
    "            print('Epoch:', str(epoch + 1), '/', str(self.num_epoch), '|', 'loss:', loss, '|',  'train accuracy:', round(y_true / X.shape[0] * 100, 2), '%', '|', 'test accuracy:', round(acc * 100, 2), '%.')\n",
    "           \n",
    "    def predict(self, X):\n",
    "        y_prob = self.__forward(X)\n",
    "        vfunc = np.vectorize(lambda x: 1 if x > 0.5 else 0)\n",
    "        return vfunc(y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {"autoscroll" : 1},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 1000 | loss: 2.363118186125232 | train accuracy: 61.54 % | test accuracy: 66.67 %.\n",
      "Epoch: 2 / 1000 | loss: 2.2936886741316957 | train accuracy: 61.54 % | test accuracy: 66.67 %.\n",
      "Epoch: 3 / 1000 | loss: 2.2292095182952765 | train accuracy: 61.54 % | test accuracy: 66.67 %.\n",
      "Epoch: 4 / 1000 | loss: 2.1753585508745323 | train accuracy: 69.23 % | test accuracy: 66.67 %.\n",
      "Epoch: 5 / 1000 | loss: 2.1344719858962464 | train accuracy: 69.23 % | test accuracy: 66.67 %.\n",
      "...\n"
      
     ]
    },
  
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     
      "Epoch: 995 / 1000 | loss: 0.0556951382895211 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n",
      "Epoch: 996 / 1000 | loss: 0.05564351589891698 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n",
      "Epoch: 997 / 1000 | loss: 0.05559198365795457 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n",
      "Epoch: 998 / 1000 | loss: 0.05554054133036015 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n",
      "Epoch: 999 / 1000 | loss: 0.05548918868070455 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n",
      "Epoch: 1000 / 1000 | loss: 0.05543792547439876 | train accuracy: 100.0 % | test accuracy: 100.0 %.\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(2)\n",
    "model.train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNhb3cie-a5w"
   },
   "source": [
    "#### Perceptron w PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eI-gM4vha-YO"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class PerceptronTorch(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)     # jedna warstwa modelu - liniowa\n",
    "        self.sigm = nn.Sigmoid()                           # funkcja aktywacji sigmoidalna\n",
    "\n",
    "    def forward(self, x): \n",
    "        out = self.linear(x)                               # obliczanie wyjÅ›cia\n",
    "        out = self.sigm(out)                               # przeksztalcenie sigmoidalne\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron_torch():\n",
    "    \n",
    "    # Params\n",
    "    torch.manual_seed(717)\n",
    "    epochs = 1000\n",
    "    lr_rate = 0.01\n",
    "    \n",
    "    # Model\n",
    "    model = PerceptronTorch(input_dim = 2, output_dim = 1)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = lr_rate)       \n",
    "    \n",
    "    # Loss\n",
    "    criterion = nn.MSELoss()                                             # funkcja mse jest analogiczna do funkcji straty\n",
    "                                                                         # z modelu pierwszego tylko nie przemnozona przez 0.5 co nie ma wplywu na gradient\n",
    "    # Train data\n",
    "    inputs = torch.from_numpy(X_train).view(-1,2).float()                # konwersja do float\n",
    "    labels = torch.from_numpy(y_train).float()                           # konwersja do float\n",
    "    \n",
    "    # Test data\n",
    "    inputs_test = torch.from_numpy(X_test).view(-1,2).float()            # konwersja do float\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(1, epochs + 1, 1):\n",
    "        \n",
    "        optimizer.zero_grad()                                            # wyczyszczenie gradientu\n",
    "        outputs = model(inputs)                                          # przeksztalcenie danych\n",
    "        \n",
    "        loss = criterion(outputs, labels.unsqueeze(1))                   # obliczenie loss\n",
    "        loss.backward()                                                  \n",
    "        \n",
    "        optimizer.step()                                                 # aktualizacja wag\n",
    "        \n",
    "        # Eval train\n",
    "        y_prob = model(inputs).detach().numpy()                          # predykcja\n",
    "        y_pred = [1 if y_ > 0.5 else 0 for y_ in y_prob]                 # obliczenie labels gdzie prog ustalam na 0.5\n",
    "        acc_train = sum([ y == y_ for y, y_ in zip(y_pred, y_train)]) / y_train.shape[0]\n",
    "        \n",
    "        # Evala test\n",
    "        y_prob_test = model(inputs_test).detach().numpy()                # predykcja\n",
    "        y_pred_test = [1 if y_ > 0.5 else 0 for y_ in y_prob_test]       # obliczenie labels gdzie prog ustalam na 0.5\n",
    "        acc_test= sum([ y == y_ for y, y_ in zip(y_pred_test, y_test)]) / y_test.shape[0]\n",
    "        \n",
    "        # Print results\n",
    "        print('epoch {} | loss {} | accuracy train {}% | accuracy test {}%'.format(epoch, loss.item(), round(acc_train * 100, 2), round(acc_test * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 0.643459677696228 | accuracy train 30.77% | accuracy test 33.33%\n",
      "epoch 2 | loss 0.6429001688957214 | accuracy train 30.77% | accuracy test 33.33%\n",
      "epoch 3 | loss 0.6423274278640747 | accuracy train 30.77% | accuracy test 33.33%\n",
      "epoch 4 | loss 0.6417413353919983 | accuracy train 30.77% | accuracy test 33.33%\n",
      "...\n"
     ]
    },
    
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 995 | loss 0.020326267927885056 | accuracy train 100.0% | accuracy test 100.0%\n",
      "epoch 996 | loss 0.020314179360866547 | accuracy train 100.0% | accuracy test 100.0%\n",
      "epoch 997 | loss 0.02030211314558983 | accuracy train 100.0% | accuracy test 100.0%\n",
      "epoch 998 | loss 0.020290056243538857 | accuracy train 100.0% | accuracy test 100.0%\n",
      "epoch 999 | loss 0.020278016105294228 | accuracy train 100.0% | accuracy test 100.0%\n",
      "epoch 1000 | loss 0.020266003906726837 | accuracy train 100.0% | accuracy test 100.0%\n"
     ]
    }
   ],
   "source": [
    "train_perceptron_torch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WUM2z.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
